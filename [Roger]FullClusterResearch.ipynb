{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is the workstation for develop best cluster method and number of clusters to use. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libs\n",
    "from sklearn import preprocessing\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import AffinityPropagation\n",
    "from sklearn.cluster import MeanShift\n",
    "from sklearn.cluster import SpectralClustering\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.cluster import Birch\n",
    "\n",
    "from sklearn import metrics\n",
    "from scipy.stats import gaussian_kde\n",
    "from scipy import stats\n",
    "from scipy.spatial.distance import cdist, pdist, euclidean\n",
    "from sklearn.decomposition import PCA\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import collections\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1459, 80)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = pd.read_csv('./data/test.csv')\n",
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('./data/train_120feats_Dense_OutlierFree_LogTransform.csv')\n",
    "test = pd.read_csv('./data/test_119feats_Dense_OutlierFree_LogTransform.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPCAcomponent(df, type = \"Train\", threshold = 0.9):\n",
    "    \"\"\"\n",
    "    Returns returns the PCA fit matrix of the input dataframe, \n",
    "    using minimum PCAs that can meet the threshold requirement\n",
    "    \"\"\"\n",
    "\n",
    "    # Check if df is a valid dataframe\n",
    "    if not isinstance(df, pd.DataFrame):\n",
    "        raise TypeError('The input is not a valid dataFrame.')\n",
    "\n",
    "    # Check if the threshold is within 1\n",
    "\n",
    "    # Make adjustment if necessary, also check input type is valid\n",
    "    if type == \"Train\":\n",
    "        # Remove SalePrice before PCA.\n",
    "        try:\n",
    "            df.drop(['SalePrice'], axis = 1, inplace = True)\n",
    "        except:\n",
    "            print(\"There was no 'SalePrice' to drop in the 'train', continue....\")\n",
    "    elif type == \"Test\":\n",
    "        pass\n",
    "    else:\n",
    "        raise ValueError(\"Type has to be 'Train' or 'Test'.\")\n",
    "\n",
    "    # Scale the data\n",
    "    dfToScaleMatrix = preprocessing.scale(df.as_matrix())\n",
    "\n",
    "    # Perform PCA\n",
    "    pca = PCA().fit(dfToScaleMatrix)\n",
    "\n",
    "    # Get incremental and cumulative viariance explained by PCA.\n",
    "    increVarExpl = pca.explained_variance_ratio_\n",
    "    totlSumVar = np.array([sum(increVarExpl[0:i+1]) for i,x in enumerate(increVarExpl)]) \n",
    "\n",
    "    # Get the num of PCA needed.\n",
    "    for i, val in enumerate(totlSumVar):\n",
    "        if val >= threshold:\n",
    "            numPCA = i\n",
    "            VarExp = val\n",
    "            break\n",
    "    # Present result\n",
    "    print(\"We can use {0} PCAs to explain {1:.4f} variance.\".format(numPCA, VarExp))\n",
    "    # Fit transform the df\n",
    "    result = PCA(n_components=numPCA).fit_transform(dfToScaleMatrix) \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We can use 54 PCAs to explain 0.9045 variance.\n"
     ]
    }
   ],
   "source": [
    "pcaFitMatrix = getPCAcomponent(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getBestClusterNum(pcaFitMatrix, maxCluster=30, method = \"KMeans\"):\n",
    "    \"\"\"\n",
    "    Returns the best number of clusters to use based on different distancce based scores\n",
    "    \"\"\"\n",
    "\n",
    "    # Num of clusters to consider\n",
    "    clustersNumList = range(2,maxCluster)\n",
    "\n",
    "    # Score distance metric\n",
    "    distanceList = ['cityblock', 'cosine', 'euclidean', 'l1', 'l2', 'manhattan']\n",
    "\n",
    "    # Cluster method\n",
    "    clusterMethods = {'KMeans': KMeans(),\n",
    "                      'Affinity propagation': AffinityPropagation(), \n",
    "                      'Mean-shift': MeanShift(), \n",
    "                      'Spectral clustering': SpectralClustering(),  \n",
    "                      'Agglomerative clustering': AgglomerativeClustering(), \n",
    "                      'DBSCAN': DBSCAN(), \n",
    "                      'Gaussian mixtures': GaussianMixture(), \n",
    "                      'Birch': Birch()}\n",
    "\n",
    "    # Create cluster model object\n",
    "    if method not in clusterMethods:\n",
    "        raise TypeError(\"{0} is not a valid clustering method.\".format(method))\n",
    "    else:\n",
    "        clusterModel = clusterMethods[method]\n",
    "\n",
    "    # Use the cluster model object to perform clustering\n",
    "    if method == \"KMeans\":\n",
    "        \n",
    "        fitResult = clusterModel(n_clusters=i).fit(pcaFitMatrix)\n",
    "    \n",
    "    \n",
    "    fit_results_list = []\n",
    "    for i in clusters:\n",
    "        fit_result = KMeans(n_clusters=i, init='k-means++', n_init=50, max_iter=2000).fit(full_dataset_after_PCA)\n",
    "        fit_results_list.append(fit_result)\n",
    "\n",
    "    # Calculate silhouette score and \n",
    "    sil_score_list = []\n",
    "    calinski_harabaz_score_list = []\n",
    "    for item in fit_results_list:\n",
    "        label = item.labels_\n",
    "        sil_score = metrics.silhouette_score(full_dataset_after_PCA,label, metric='euclidean')\n",
    "        calinski_harabaz_score = metrics.calinski_harabaz_score(full_dataset_after_PCA,label)  \n",
    "        sil_score_list.append(sil_score)\n",
    "        calinski_harabaz_score_list.append(calinski_harabaz_score)\n",
    "\n",
    "    plt.figure(1)\n",
    "    plt.plot(clusters, sil_score_list,'o-')\n",
    "    #plt.axis([1, 28, 0.1, 0.4])\n",
    "    plt.xlabel('Number of Clusters');\n",
    "    plt.ylabel('Silhouette Score');\n",
    "\n",
    "    plt.figure(2)\n",
    "    plt.plot(clusters, calinski_harabaz_score_list,'o-')\n",
    "    #plt.axis([1, 28, 20, 50])\n",
    "    plt.xlabel('Number of Clusters');\n",
    "    plt.ylabel('Calinski-Harabaz Index');"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
